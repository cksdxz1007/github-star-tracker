# main.py
import os
import time
import requests
import pandas as pd
from datetime import datetime, timezone
from dotenv import load_dotenv

# LangChain å¯¼å…¥
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# 2. æ£€æŸ¥é…ç½®
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
GITHUB_USERNAME = os.getenv("GITHUB_USERNAME")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "gpt-3.5-turbo")

if not all([GITHUB_TOKEN, GITHUB_USERNAME, OPENAI_API_KEY]):
    print("âŒ é”™è¯¯: è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®æ‰€æœ‰å¿…è¦çš„ç¯å¢ƒå˜é‡ã€‚")
    exit(1)

HEADERS = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

# ==================== åŠŸèƒ½æ¨¡å—ï¼šæ•°æ®è·å– ====================

def fetch_starred_repos():
    """è·å–æ‰€æœ‰ Star çš„ä»“åº“åˆ—è¡¨"""
    repos = []
    page = 1
    print(f"ğŸ“¡ å¼€å§‹è·å–ç”¨æˆ· {GITHUB_USERNAME} çš„ Star åˆ—è¡¨...")
    
    while True:
        url = f"https://api.github.com/users/{GITHUB_USERNAME}/starred?per_page=100&page={page}"
        try:
            resp = requests.get(url, headers=HEADERS)
            resp.raise_for_status()
            data = resp.json()
            if not data: break
            
            repos.extend(data)
            print(f"   å·²åŠ è½½ç¬¬ {page} é¡µï¼Œç´¯è®¡ {len(repos)} ä¸ª...")
            page += 1
            time.sleep(0.2) # ç¨å¾®é™åˆ¶é€Ÿç‡
        except Exception as e:
            print(f"âŒ API è¯·æ±‚å¤±è´¥: {e}")
            break
    return repos

def get_commit_activity(repo_full_name):
    """è·å–è¿‡å»ä¸€å¹´çš„æäº¤ç»Ÿè®¡ (Commit Frequency)"""
    url = f"https://api.github.com/repos/{repo_full_name}/stats/participation"
    try:
        resp = requests.get(url, headers=HEADERS)
        if resp.status_code == 200:
            data = resp.json()
            if 'all' in data:
                return sum(data['all']) # è¿”å›è¿‡å»52å‘¨çš„æ€»æäº¤æ•°
    except:
        pass
    return 0

def get_latest_commit_msg(repo_full_name, branch):
    """è·å–æœ€åä¸€æ¬¡æäº¤çš„ Message"""
    url = f"https://api.github.com/repos/{repo_full_name}/commits/{branch}"
    try:
        resp = requests.get(url, headers=HEADERS)
        if resp.status_code == 200:
            msg = resp.json()['commit']['message']
            return msg.split('\n')[0][:100] # åªå–ç¬¬ä¸€è¡Œå‰100å­—
    except:
        pass
    return "æ— æ³•è·å–"

def get_readme_summary(repo_full_name):
    """ä»READMEæ–‡ä»¶ä¸­ä½¿ç”¨LLMæ€»ç»“é¡¹ç›®ç”¨é€”"""
    try:
        # é¦–å…ˆå°è¯•è·å–README.md
        url = f"https://api.github.com/repos/{repo_full_name}/readme"
        resp = requests.get(url, headers=HEADERS)

        if resp.status_code == 200:
            import base64
            readme_content = base64.b64decode(resp.json()['content']).decode('utf-8')

            # æå–å‰1500ä¸ªå­—ç¬¦ä½œä¸ºå‚è€ƒæ–‡æœ¬ï¼ˆç»™LLMæ›´å¤šå†…å®¹ï¼‰
            summary_text = readme_content[:1500]

            # ä½¿ç”¨LLMæ€»ç»“
            summary_prompt = f"""
            è¯·é˜…è¯»ä»¥ä¸‹GitHubé¡¹ç›®çš„READMEå†…å®¹ï¼Œç”¨1-2å¥è¯æ€»ç»“è¿™ä¸ªé¡¹ç›®çš„ä¸»è¦ç”¨é€”å’ŒåŠŸèƒ½ã€‚
            è¦æ±‚ï¼š
            1. è¯­è¨€ç®€æ´æ˜äº†ï¼Œçªå‡ºé¡¹ç›®æ ¸å¿ƒåŠŸèƒ½
            2. å­—æ•°æ§åˆ¶åœ¨50å­—ä»¥å†…
            3. ç›´æ¥è¯´æ˜é¡¹ç›®æ˜¯ä»€ä¹ˆ/åšä»€ä¹ˆï¼Œä¸éœ€è¦è¯´æ˜å¦‚ä½•ä½¿ç”¨

            READMEå†…å®¹ï¼š
            {summary_text}

            æ€»ç»“ï¼š
            """

            try:
                # é…ç½®ç®€åŒ–çš„LLMç”¨äºæ€»ç»“
                summary_llm = ChatOpenAI(
                    model_name=LLM_MODEL_NAME,
                    temperature=0.3,
                    openai_api_key=OPENAI_API_KEY,
                    openai_api_base=OPENAI_API_BASE
                )

                chain = summary_prompt | summary_llm | StrOutputParser()
                result = chain.invoke({})

                if result and len(result.strip()) > 5:
                    return result.strip()
                return "æ— æè¿°"

            except Exception as llm_error:
                print(f"   âš ï¸ LLMæ€»ç»“å¤±è´¥: {str(llm_error)[:50]}")
                # å¦‚æœLLMå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æå–
                return get_readme_simple(summary_text)

        return "æ— æè¿°"

    except Exception as e:
        return "æ— æè¿°"

def get_readme_simple(readme_content):
    """ç®€å•çš„READMEæå–ï¼ˆä½œä¸ºLLMçš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
    lines = readme_content.split('\n')[:30]
    title_lines = []
    description_lines = []

    for line in lines:
        line = line.strip()
        if line.startswith('#'):
            title_lines.append(line.lstrip('#').strip())
        elif len(line) > 30 and not line.startswith('!['):
            description_lines.append(line)

    result_parts = []

    if title_lines:
        main_title = title_lines[0]
        if len(main_title) < 100:
            result_parts.append(main_title)

    for desc in description_lines[:3]:
        if 20 < len(desc) < 150:
            result_parts.append(desc)
            break

    if result_parts:
        result = ' | '.join(result_parts)
        if len(result) > 200:
            result = result[:200] + "..."
        return result

    return "æ— æè¿°"

# ==================== åŠŸèƒ½æ¨¡å—ï¼šæ™ºèƒ½åˆ†æ ====================

def analyze_with_llm(df):
    """ä½¿ç”¨ LangChain åˆ†ææ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š"""
    print("\nğŸ§  æ•°æ®å¤„ç†å®Œæ¯•ï¼Œæ­£åœ¨é€šè¿‡ LLM ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š...")
    
    # 1. å‡†å¤‡ä¸Šä¸‹æ–‡ (Context Extraction)
    total_count = len(df)
    inactive_1yr = len(df[df['æ²‰å¯‚å¤©æ•°'] > 365])
    active_30d = len(df[df['æ²‰å¯‚å¤©æ•°'] < 30])
    
    # é€‰å–è¿‘æœŸæœ€æ´»è·ƒçš„ Top 5
    top_active = df.sort_values("æ²‰å¯‚å¤©æ•°").head(5)
    active_str = "\n".join(
        [f"- [{row['ä»“åº“å']}]({row['ä»“åº“é“¾æ¥']}) - [{row['ç¼–ç¨‹è¯­è¨€']}] - {row['é¡¹ç›®æè¿°']}\n  æ›´æ–°äº{row['æœ€è¿‘æ›´æ–°æ—¥æœŸ']}ï¼Œæ›´æ–°å†…å®¹: {row['æœ€è¿‘æ›´æ–°å†…å®¹']}"
         for _, row in top_active.iterrows()]
    )

    # é€‰å–å·²ç»"æ­»æ‰"ä½†è¿˜æœ‰åæ°”çš„ Top 5 (Staræ•°é«˜ä½†å¾ˆä¹…æ²¡æ›´)
    dead_giants = df[df['æ²‰å¯‚å¤©æ•°'] > 365].sort_values("Staræ•°", ascending=False).head(5)
    dead_str = "\n".join(
        [f"- [{row['ä»“åº“å']}]({row['ä»“åº“é“¾æ¥']}) - [{row['ç¼–ç¨‹è¯­è¨€']}] - {row['é¡¹ç›®æè¿°']}\n  å·²åœæ›´{row['æ²‰å¯‚å¤©æ•°']}å¤© (Star: {row['Staræ•°']})"
         for _, row in dead_giants.iterrows()]
    )

    # 2. æ„å»º Prompt
    template = """
    ä½ æ˜¯ä¸€åæŠ€æœ¯èµ„äº§ç®¡ç†ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„ GitHub Star æ•°æ®ç”Ÿæˆä¸€ä»½ç®€æŠ¥ã€‚

    ã€æ•°æ®æ¦‚è§ˆã€‘
    - å…³æ³¨é¡¹ç›®æ€»æ•°: {total_count}
    - æåº¦æ´»è·ƒé¡¹ç›®(è¿‘30å¤©): {active_30d}
    - ç–‘ä¼¼åºŸå¼ƒé¡¹ç›®(è¶…1å¹´): {inactive_1yr}

    ã€è¿‘æœŸæ´»è·ƒç„¦ç‚¹ã€‘
    {active_str}

    ã€å¦‚æœæ˜¯é‡ä¾èµ–ï¼Œéœ€è­¦æƒ•çš„é•¿æœŸæœªæ›´é¡¹ç›®ã€‘
    {dead_str}

    ã€ä»»åŠ¡ã€‘
    è¯·ç”Ÿæˆä¸€ä»½ Markdown æ ¼å¼çš„åˆ†ææŠ¥å‘Šã€‚
    1. ç»™å‡ºä¸€ä¸ªå…³äºç”¨æˆ·å…³æ³¨æŠ€æœ¯æ ˆçš„æ•´ä½“å¥åº·åº¦è¯„åˆ†ï¼ˆ0-10åˆ†ï¼‰å’Œç®€çŸ­è¯„ä»·ã€‚
    2. å¯¹"è¿‘æœŸæ´»è·ƒç„¦ç‚¹"ä¸­çš„æ¯ä¸ªé¡¹ç›®ï¼Œæ ¹æ®å…¶ç¼–ç¨‹è¯­è¨€å’Œé¡¹ç›®æè¿°ï¼Œè¯´æ˜è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„é¡¹ç›®ï¼Œç„¶åå¯¹å…¶æ›´æ–°å†…å®¹è¿›è¡ŒæŠ€æœ¯è§£è¯»ï¼ˆæ¨æµ‹å®ƒæ˜¯åœ¨ä¿®Bugè¿˜æ˜¯å‘æ–°ç‰ˆï¼‰ã€‚
    3. å¯¹"é•¿æœŸæœªæ›´é¡¹ç›®"ç»™å‡ºè¡ŒåŠ¨å»ºè®®ï¼ˆå¦‚ï¼šå»ºè®®å¯»æ‰¾æ›¿ä»£å“ï¼‰ï¼Œå¹¶è¯´æ˜å¦‚æœè¿™äº›é¡¹ç›®å¯¹ä½ çš„å·¥ä½œå¾ˆé‡è¦åº”è¯¥æ€ä¹ˆåŠã€‚
    4. ä¿æŒè¯­æ°”ä¸“ä¸šã€å®¢è§‚ã€‚æ¯ä¸ªå»ºè®®éƒ½è¦å…·ä½“å¯è¡Œã€‚
    5. åœ¨æŠ¥å‘Šä¸­é€‚å½“æåŠç¼–ç¨‹è¯­è¨€ä¿¡æ¯ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æŠ€æœ¯æ ˆåˆ†å¸ƒã€‚
    """
    
    prompt = PromptTemplate.from_template(template)
    
    # 3. é…ç½®æ¨¡å‹
    llm = ChatOpenAI(
        model_name=LLM_MODEL_NAME,
        temperature=0.6, 
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_API_BASE
    )
    
    # 4. æ‰§è¡Œ (LCEL è¯­æ³•)
    chain = prompt | llm | StrOutputParser()
    
    return chain.invoke({
        "total_count": total_count,
        "active_30d": active_30d,
        "inactive_1yr": inactive_1yr,
        "active_str": active_str,
        "dead_str": dead_str
    })

# ==================== ä¸»ç¨‹åº ====================

def main():
    print("\n" + "="*50)
    print("ğŸš€ GitHub Star Tracker å¼€å§‹è¿è¡Œ")
    print("="*50)

    # æ­¥éª¤1: è·å–åŸºç¡€åˆ—è¡¨
    print("\nğŸ“¡ [æ­¥éª¤ 1/4] æ­£åœ¨è·å– GitHub Starred ä»“åº“åˆ—è¡¨...")
    repos = fetch_starred_repos()
    if not repos:
        print("âŒ æœªè·å–åˆ°ä»»ä½•ä»“åº“ï¼Œç¨‹åºé€€å‡º")
        return

    processed_data = []
    now = datetime.now(timezone.utc)

    print(f"\nâš™ï¸ [æ­¥éª¤ 2/4] æ­£åœ¨æ·±å…¥åˆ†æ {len(repos)} ä¸ªä»“åº“...")
    print("   - æ”¶é›†ä»“åº“åŸºæœ¬ä¿¡æ¯ï¼ˆæè¿°ã€é“¾æ¥ã€æ˜Ÿæ ‡æ•°ï¼‰")
    print("   - è®¡ç®—æ²‰å¯‚å¤©æ•°å’Œæ´»åŠ¨çŠ¶æ€")
    print("   - è·å–è¿‘æœŸæ´»è·ƒä»“åº“çš„æäº¤æ•°æ®")

    # âš ï¸ è°ƒè¯•æ¨¡å¼ï¼šåªå–å‰ 30 ä¸ªä»“åº“ä»¥èŠ‚çœæ—¶é—´ã€‚æ­£å¼ä½¿ç”¨è¯·å»æ‰ [:30]
    target_repos = repos # acts on all repos
    # target_repos = repos[:30]

    start_time = time.time()

    for i, repo in enumerate(target_repos):
        pushed_at = repo['pushed_at']
        repo_name = repo['full_name']
        
        days_inactive = -1
        date_str = "N/A"
        
        if pushed_at:
            dt = datetime.strptime(pushed_at, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
            days_inactive = (now - dt).days
            date_str = dt.strftime("%Y-%m-%d")
            
        # ä¼˜åŒ–ç­–ç•¥ï¼šåªæœ‰æœ€è¿‘åŠå¹´æœ‰æ›´æ–°çš„é¡¹ç›®ï¼Œæ‰å»æŸ¥ commit frequencyï¼ŒèŠ‚çœ API
        commits_last_year = 0
        last_msg = ""

        if days_inactive != -1 and days_inactive < 180:
            commits_last_year = get_commit_activity(repo_name)
            last_msg = get_latest_commit_msg(repo_name, repo['default_branch'])

        # è·å–é¡¹ç›®æè¿°å’Œé“¾æ¥
        description = repo.get('description', 'æ— æè¿°') or 'æ— æè¿°'

        # å¦‚æœæ²¡æœ‰æè¿°ï¼Œå°è¯•ä»READMEè·å–
        if not description or description == 'æ— æè¿°':
            print(f"   ğŸ“„ æ­£åœ¨è·å– {repo_name} çš„READMEå†…å®¹...")
            description = get_readme_summary(repo_name)
            time.sleep(0.1)  # ç¨å¾®é™åˆ¶é€Ÿç‡

        html_url = repo.get('html_url', '')
        language = repo.get('language', 'Unknown') or 'Unknown'

        processed_data.append({
            "ä»“åº“å": repo_name,
            "ç¼–ç¨‹è¯­è¨€": language,
            "é¡¹ç›®æè¿°": description,
            "ä»“åº“é“¾æ¥": html_url,
            "Staræ•°": repo['stargazers_count'],
            "æœ€è¿‘æ›´æ–°æ—¥æœŸ": date_str,
            "æ²‰å¯‚å¤©æ•°": days_inactive,
            "å¹´æäº¤æ•°": commits_last_year,
            "æœ€è¿‘æ›´æ–°å†…å®¹": last_msg
        })
        
        if (i+1) % 10 == 0:
            elapsed = time.time() - start_time
            print(f"   âœ“ å·²å¤„ç† {i+1}/{len(target_repos)} ä¸ªä»“åº“ (ç”¨æ—¶ {elapsed:.1f}s)")

    elapsed_total = time.time() - start_time
    print(f"   âœ“ ä»“åº“åˆ†æå®Œæˆ! æ€»ç”¨æ—¶ {elapsed_total:.1f}s")

    # æ­¥éª¤3: ä¿å­˜ CSV
    print(f"\nğŸ’¾ [æ­¥éª¤ 3/4] æ­£åœ¨ä¿å­˜åŸå§‹æ•°æ®åˆ° CSV æ–‡ä»¶...")

    # åˆ›å»º DataFrame
    df = pd.DataFrame(processed_data)

    # ç»Ÿè®¡å„è¯­è¨€çš„ä»“åº“æ•°é‡
    language_counts = df['ç¼–ç¨‹è¯­è¨€'].value_counts()
    print(f"   ğŸ“Š é¡¹ç›®è¯­è¨€åˆ†å¸ƒ: {dict(language_counts)}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    csv_dir = "csv_output"
    os.makedirs(csv_dir, exist_ok=True)

    # ä¿å­˜å®Œæ•´çš„CSV
    csv_filename = f"{csv_dir}/github_stars_{timestamp}.csv"
    df.to_csv(csv_filename, index=False, encoding="utf-8-sig")
    print(f"   âœ“ å®Œæ•´æ•°æ®å·²ä¿å­˜: {csv_filename}")

    # ç”Ÿæˆè¯­è¨€ç»Ÿè®¡æ‘˜è¦
    summary_file = f"{csv_dir}/language_summary_{timestamp}.txt"
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write(f"GitHub Stars é¡¹ç›®è¯­è¨€åˆ†ç±»æ‘˜è¦\n")
        f.write(f"{'='*50}\n\n")
        f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»é¡¹ç›®æ•°: {len(df)}\n\n")
        f.write(f"è¯­è¨€åˆ†å¸ƒ:\n")
        for lang, count in language_counts.items():
            f.write(f"  {lang}: {count} ä¸ªé¡¹ç›® ({count/len(df)*100:.1f}%)\n")

    print(f"   âœ“ è¯­è¨€ç»Ÿè®¡æ‘˜è¦: {summary_file}")
    print(f"   âœ“ CSVæ–‡ä»¶ä¿å­˜åœ¨ç›®å½•: {csv_dir}/")

    # æ­¥éª¤4: ç”Ÿæˆ AI æŠ¥å‘Š
    print(f"\nğŸ§  [æ­¥éª¤ 4/4] æ­£åœ¨é€šè¿‡ LLM ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š...")
    print("   - åˆ†ææ´»è·ƒé¡¹ç›®å’Œåœæ»é¡¹ç›®")
    print("   - ç”Ÿæˆå¥åº·åº¦è¯„åˆ†å’ŒæŠ€æœ¯å»ºè®®")

    # åˆ›å»ºæŠ¥å‘Šç›®å½•
    reports_dir = "reports"
    os.makedirs(reports_dir, exist_ok=True)

    try:
        report = analyze_with_llm(df)
        md_filename = f"{reports_dir}/analysis_report_{timestamp}.md"
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write(report)

        print(f"   âœ“ AI åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {md_filename}")
        print("\n" + "="*50)
        print("ğŸ“Š åˆ†ææŠ¥å‘Šå†…å®¹:")
        print("="*50)
        print(report)
        print("\n" + "="*50)
        print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        print("="*50)
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
        print(f"   ğŸ“Š CSVæ–‡ä»¶: {csv_dir}/")
        print(f"   ğŸ“ æŠ¥å‘Šæ–‡ä»¶: {reports_dir}/")
        print("="*50)
    except Exception as e:
        print(f"âŒ AI åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

